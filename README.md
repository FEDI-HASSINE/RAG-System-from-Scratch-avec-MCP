# ğŸ” RAG System from Scratch avec MCP

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.128-green.svg)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Un systÃ¨me **RAG (Retrieval-Augmented Generation)** complet, construit de zÃ©ro avec architecture **MCP (Model Context Protocol)**.

## âœ¨ FonctionnalitÃ©s

- ğŸ” **Recherche sÃ©mantique** avec Pinecone (FAISS en secours) et SentenceTransformers
- ğŸ“Š **Reranking** avec Cross-Encoder pour une pertinence maximale
- ğŸ¤– **Agent RAG** orchestrant le pipeline complet
- ğŸ”Œ **API MCP** unifiÃ©e pour tous les outils
- ğŸ›¡ï¸ **Zero hallucination** â€” rÃ©ponses basÃ©es uniquement sur les documents
- ğŸ“ˆ **Ã‰valuation LLM-as-a-Judge** avec mÃ©triques standardisÃ©es
- ğŸ’» **CLI + UI Streamlit** pour la dÃ©mo

## ğŸš€ DÃ©marrage rapide

### 1. Cloner et installer
```bash
git clone https://github.com/FEDI-HASSINE/RAG-System-from-Scratch-avec-MCP.git
cd RAG-System-from-Scratch-avec-MCP
python -m venv .venv && source .venv/bin/activate
pip install -r rag_system/requirements-phase2.txt  # inclut ingestion + Pinecone + Streamlit
```

### 2. Variables d'environnement (Pinecone + LLM)
```bash
export PINECONE_API_KEY="..."
export PINECONE_INDEX="rag-index"     # existant dans votre compte
export PINECONE_NAMESPACE="demo"      # changez selon vos donnÃ©es
export OPENAI_API_KEY="..."           # ou autre LLM compatible
```

### 3. Ingestion + indexation Pinecone
```bash
source .venv/bin/activate
python rag_system/run_ingestion.py               # chunking (Chonkie activÃ© si installÃ©)
python rag_system/run_indexing_pinecone.py       # envoie les embeddings vers Pinecone
```

> Notes :
> - Chonkie est dÃ©jÃ  installÃ© dans l'environnement de dÃ©mo ; si vous rÃ©installez ailleurs, installez-le en option (`pip install chonkie==1.5.2 --no-deps`) puis gardez numpy < 2 pour compatibilitÃ© torch CPU.
> - Pour rafraÃ®chir les donnÃ©es, rejouez simplement ingestion puis indexation ; le namespace Pinecone (`PINECONE_NAMESPACE`) permet de sÃ©parer vos ensembles de documents.

### 4. DÃ©marrer le serveur MCP
```bash
cd rag_system/mcp_server
uvicorn main:app --reload --port 8000
```

### 5. Tester via CLI (MCP client)
```bash
cd rag_system/demo
python rag_cli.py ask "What is system architecture?" --top-k 3
```

### 6. Lancer l'interface web
```bash
cd rag_system/demo
streamlit run app.py
# Ouvrez http://localhost:8501
```

## ğŸ“¦ Architecture

```
RAG-System-from-Scratch-avec-MCP/
â””â”€â”€ rag_system/
    â”œâ”€â”€ data/              # Phase 1: Ingestion & chunking
    â”œâ”€â”€ embeddings/        # Phase 2: Vectorisation
    â”œâ”€â”€ vector_store/      # Phase 2: Index Pinecone (FAISS fallback)
    â”œâ”€â”€ mcp_server/        # Phase 5: API MCP
    â”‚   â””â”€â”€ tools/         # Phases 3-4: Outils RAG
    â”œâ”€â”€ agents/            # Phase 6: Agent RAG
    â”œâ”€â”€ evaluation/        # Phase 7: MÃ©triques
    â””â”€â”€ demo/              # Phase 8: CLI + UI
```

## ğŸ”„ Pipeline RAG

```
Question
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Embed      â”‚ â”€â”€â–¶ â”‚   Retrieve   â”‚ â”€â”€â–¶ â”‚   Rerank     â”‚
â”‚   (384 dims) â”‚     â”‚ (Pinecone)   â”‚     â”‚ (CrossEnc)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                                                â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   RÃ©ponse    â”‚ â—€â”€â”€ â”‚   LLM        â”‚
                     â”‚   + Sources  â”‚     â”‚   Generate   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Technologies

| Composant | Technologie |
|-----------|-------------|
| Embeddings | SentenceTransformers `all-MiniLM-L6-v2` |
| Vector Store | Pinecone (`rag-index` / namespace configurable) â€” FAISS en secours |
| Reranking | Cross-Encoder `ms-marco-MiniLM-L-6-v2` |
| API | FastAPI + MCP Protocol |
| LLM | OpenAI / Ollama / Mock |
| CLI | Typer + Rich |
| UI | Streamlit |
| Evaluation | LLM-as-a-Judge |

## ğŸ“Š MÃ©triques d'Ã©valuation

| MÃ©trique | Description |
|----------|-------------|
| **Groundedness** | RÃ©ponse basÃ©e sur le contexte |
| **Relevance** | RÃ©pond Ã  la question |
| **Faithfulness** | Pas d'hallucination |

## ğŸ“– Documentation

Chaque dossier contient un `README.md` dÃ©taillÃ© :
- [rag_system/](rag_system/README.md) â€” Vue d'ensemble
- [data/](rag_system/data/README.md) â€” Ingestion
- [embeddings/](rag_system/embeddings/README.md) â€” Vectorisation
- [vector_store/](rag_system/vector_store/README.md) â€” Stockage FAISS
- [mcp_server/](rag_system/mcp_server/README.md) â€” API MCP
- [agents/](rag_system/agents/README.md) â€” Agent RAG
- [evaluation/](rag_system/evaluation/README.md) â€” MÃ©triques
- [demo/](rag_system/demo/README.md) â€” Interfaces

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©ez votre branche (`git checkout -b feature/amazing-feature`)
3. Commit (`git commit -m 'Add amazing feature'`)
4. Push (`git push origin feature/amazing-feature`)
5. Ouvrez une Pull Request

## ğŸ“„ License

MIT License â€” voir [LICENSE](LICENSE) pour plus de dÃ©tails.

---

Fait avec â¤ï¸ par [FEDI-HASSINE](https://github.com/FEDI-HASSINE)