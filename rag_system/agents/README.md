# ğŸ¤– Agents â€” RAG Agent & LLM (Phase 6)

## ğŸ¯ Objectif

Orchestrer le pipeline RAG complet : rÃ©cupÃ©ration â†’ reranking â†’ gÃ©nÃ©ration de rÃ©ponse.

## ğŸ”§ ProblÃ¨mes rÃ©solus

| ProblÃ¨me | Solution |
|----------|----------|
| Appels MCP manuels | `mcp_client.py` â€” Client HTTP avec retries |
| Multi-providers LLM | `llm_service.py` â€” OpenAI, Ollama, Mock |
| Hallucinations | `prompts.py` â€” Prompts stricts "basÃ© sur le contexte" |
| Pipeline complexe | `rag_agent.py` â€” Orchestration avec trace |
| Pas de fallback | Mock LLM si API indisponible |

## ğŸ“ Fichiers

```
agents/
â”œâ”€â”€ mcp_client.py    # Client HTTP pour MCP
â”œâ”€â”€ llm_service.py   # Service LLM unifiÃ©
â”œâ”€â”€ prompts.py       # Templates de prompts
â”œâ”€â”€ rag_agent.py     # Agent RAG principal
â””â”€â”€ requirements.txt
```

## ğŸš€ Utilisation

### RÃ©ponse simple
```python
from agents.rag_agent import rag_answer

answer = rag_answer("What are the security measures?")
print(answer)
```

### Avec trace complÃ¨te
```python
from agents.rag_agent import RAGAgent

agent = RAGAgent()
response = agent.answer("Explain the system architecture")

print(response.answer)
print(response.sources)

for step in response.trace.steps:
    print(f"{step.name}: {step.duration_ms}ms")
```

## ğŸ”„ Pipeline RAG

```
Question
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Retrieve    â”‚ â†’ MCP retrieve_chunks
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Rerank      â”‚ â†’ MCP rerank
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Build       â”‚ â†’ Prompt avec contexte
â”‚     Context     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. LLM         â”‚ â†’ OpenAI / Ollama / Mock
â”‚     Generate    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â–¼
RÃ©ponse + Sources + Trace
```

## ğŸ¤– Providers LLM

| Provider | Configuration | Usage |
|----------|---------------|-------|
| OpenAI | `OPENAI_API_KEY` | Production |
| Ollama | Local `localhost:11434` | Dev/Offline |
| Mock | Aucune | Tests |

```python
from agents.llm_service import LLMConfig, LLMProvider

config = LLMConfig(
    provider=LLMProvider.OPENAI,
    model="gpt-4o-mini",
    temperature=0.1
)
```

## ğŸ›¡ï¸ Zero Hallucination

Les prompts dans `prompts.py` imposent :
- RÃ©ponse **uniquement** basÃ©e sur le contexte fourni
- Citation explicite des sources
- Aveu d'ignorance si info absente

```python
SYSTEM_PROMPT = """
Tu es un assistant qui rÃ©pond UNIQUEMENT Ã  partir du contexte fourni.
Si l'information n'est pas dans le contexte, dis "Je n'ai pas cette information".
Ne jamais inventer de faits.
"""
```
