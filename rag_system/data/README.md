# ğŸ“‚ Data â€” Ingestion & Preprocessing (Phase 1)

## ğŸ¯ Objectif

Transformer des documents bruts (TXT, MD, PDF) en chunks structurÃ©s prÃªts pour l'embedding.

## ğŸ”§ ProblÃ¨mes rÃ©solus

| ProblÃ¨me | Solution |
|----------|----------|
| Documents de formats variÃ©s | `loaders.py` â€” Loaders unifiÃ©s (TXT, MD, PDF) |
| Texte bruitÃ© (HTML, espaces) | `cleaner.py` â€” Nettoyage et normalisation |
| Perte de structure (titres, sections) | `structure_detector.py` â€” DÃ©tection automatique des sections |
| Chunks trop grands/petits | `chunker.py` â€” DÃ©coupage intelligent avec overlap |
| Pipeline manuelle rÃ©pÃ©titive | `ingestion_pipeline.py` â€” Orchestration automatique |

## ğŸ“ Fichiers

```
data/
â”œâ”€â”€ raw_docs/              # Documents sources (TXT, MD, PDF)
â”œâ”€â”€ loaders.py             # Chargement multi-format
â”œâ”€â”€ cleaner.py             # Nettoyage du texte
â”œâ”€â”€ structure_detector.py  # DÃ©tection de structure
â”œâ”€â”€ chunker.py             # DÃ©coupage en chunks
â”œâ”€â”€ ingestion_pipeline.py  # Pipeline complÃ¨te
â””â”€â”€ chunks.json            # Sortie: chunks prÃªts pour embedding
```

## ğŸš€ Utilisation

```python
from data.ingestion_pipeline import IngestionPipeline

pipeline = IngestionPipeline()
result = pipeline.run()
print(f"Chunks crÃ©Ã©s: {result.total_chunks}")
```

Ou en ligne de commande :
```bash
cd rag_system
python run_ingestion.py
```

## ğŸ“Š Sortie

`chunks.json` contient :
```json
[
  {
    "chunk_id": "notes_001",
    "text": "Contenu du chunk...",
    "source": "notes.txt",
    "section": "Introduction",
    "tokens": 128
  }
]
```

## âš™ï¸ Configuration

Dans `ingestion_pipeline.py` :
- `chunk_size`: Taille cible des chunks (dÃ©faut: 512 tokens)
- `chunk_overlap`: Chevauchement entre chunks (dÃ©faut: 50 tokens)
- `min_chunk_size`: Taille minimale (dÃ©faut: 100 tokens)
