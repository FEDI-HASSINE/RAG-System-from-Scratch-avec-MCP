# ğŸ¬ Demo â€” Interface CLI & UI (Phase 8)

## ğŸ¯ Objectif

Permettre Ã  quiconque de tester le RAG System et comprendre son fonctionnement.

## ğŸ”§ ProblÃ¨mes rÃ©solus

| ProblÃ¨me | Solution |
|----------|----------|
| AccÃ¨s technique uniquement | `app.py` â€” UI Streamlit accessible |
| Pas de visibilitÃ© pipeline | Toggle "Voir raisonnement RAG" |
| Export des rÃ©ponses | Bouton Download Markdown |
| Tests rapides | `rag_cli.py` â€” CLI Typer |

## ğŸ“ Fichiers

```
demo/
â”œâ”€â”€ rag_cli.py      # CLI Typer
â”œâ”€â”€ app.py          # Interface Streamlit
â”œâ”€â”€ demo.md         # Documentation complÃ¨te
â””â”€â”€ screenshots/    # Captures pour portfolio
```

## ğŸš€ PrÃ©-requis

1. **MCP Server actif** :
```bash
cd rag_system/mcp_server
uvicorn main:app --reload
```

2. *(Optionnel)* ClÃ© OpenAI pour LLM rÃ©el :
```bash
export OPENAI_API_KEY=sk-...
```

## ğŸ’» Mode CLI

### Poser une question
```bash
cd rag_system/demo
python rag_cli.py ask "What is system architecture?" --top-k 3
```

### Autres commandes
```bash
python rag_cli.py health          # Ã‰tat du systÃ¨me
python rag_cli.py stats           # Statistiques
python rag_cli.py export "Q?" -o response.md
```

### Exemple de sortie
```
â“ Question: system architecture

ğŸ” Retrieved Chunks:
â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ # â”ƒ Score â”ƒ Source    â”ƒ Extrait                        â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ 1 â”‚ 5.59  â”‚ notes.txt â”‚ Technical Notes - System...    â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ§  Final Answer:
The system uses a distributed architecture with...

ğŸ“Š Pipeline Trace:
   âœ… retrieve: 50ms
   âœ… rerank: 120ms
   âœ… llm_generate: 450ms
```

## ğŸŒ Mode UI (Streamlit)

```bash
cd rag_system/demo
streamlit run app.py
# Ouvrez http://localhost:8501
```

### FonctionnalitÃ©s

| Zone | Fonction |
|------|----------|
| Input | Question utilisateur |
| Bouton | â–¶ï¸ Run RAG |
| Panel | Chunks rÃ©cupÃ©rÃ©s + scores |
| Toggle | Voir raisonnement RAG |
| Download | Export Markdown |

## ğŸ” Raisonnement RAG

Le toggle affiche les Ã©tapes :
1. **Embedding query** â€” Vectorisation
2. **Retrieving chunks** â€” Recherche FAISS
3. **Reranking** â€” Cross-encoder
4. **Prompt injection** â€” Construction contexte
5. **LLM generation** â€” RÃ©ponse finale

> âš ï¸ C'est la logique du pipeline, pas un chain-of-thought LLM.

## ğŸ”’ SÃ©curitÃ©

- ClÃ©s API via variables d'environnement (jamais affichÃ©es)
- Serveur MCP en localhost par dÃ©faut
