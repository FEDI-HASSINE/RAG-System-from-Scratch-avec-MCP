# ğŸ“Š Evaluation â€” LLM-as-a-Judge (Phase 7)

## ğŸ¯ Objectif

Mesurer objectivement la qualitÃ© du RAG Agent avec un "juge" LLM.

## ğŸ”§ ProblÃ¨mes rÃ©solus

| ProblÃ¨me | Solution |
|----------|----------|
| Ã‰valuation subjective | LLM-as-a-Judge avec scores 0-1 |
| MÃ©triques non standardisÃ©es | Groundedness, Relevance, Faithfulness |
| Suivi dans le temps | `history.csv` pour baseline |
| Visualisation | Dashboard PNG automatique |

## ğŸ“ Fichiers

```
evaluation/
â”œâ”€â”€ eval_pipeline.py    # Pipeline d'Ã©valuation
â”œâ”€â”€ eval_dataset.json   # Questions + rÃ©ponses attendues
â”œâ”€â”€ eval_results.csv    # RÃ©sultats par question (gÃ©nÃ©rÃ©)
â”œâ”€â”€ dashboard.png       # Graphique des scores (gÃ©nÃ©rÃ©)
â””â”€â”€ history.csv         # Historique des runs (gÃ©nÃ©rÃ©)
```

## ğŸš€ PrÃ©-requis

- MCP server dÃ©marrÃ©: `cd rag_system/mcp_server && uvicorn main:app --reload`
- Optionnel: `OPENAI_API_KEY` pour un Judge plus fiable (sinon Mock)
- DÃ©pendances: `pandas`, `matplotlib`

## ğŸš€ Lancer l'Ã©valuation

```bash
cd rag_system
python -c "from evaluation.eval_pipeline import run_evaluation; run_evaluation()"
```

Ou avec Python :
```python
from evaluation.eval_pipeline import run_evaluation

result = run_evaluation(
    dataset_path="evaluation/eval_dataset.json",
    output_dir="evaluation/"
)

print(f"Groundedness: {result['aggregate']['groundedness']:.2f}")
print(f"Relevance: {result['aggregate']['relevance']:.2f}")
print(f"Faithfulness: {result['aggregate']['faithfulness']:.2f}")
```

## ğŸ“‹ Format du dataset

`eval_dataset.json` :
```json
[
  {
    "question": "What is Agent2Agent protocol?",
    "expected_answer": "A2A is an open protocol enabling agents to discover and collaborate."
  }
]
```

## ğŸ“Š MÃ©triques

| MÃ©trique | Description | Score idÃ©al |
|----------|-------------|-------------|
| **Groundedness** | RÃ©ponse basÃ©e sur le contexte rÃ©cupÃ©rÃ© | 1.0 |
| **Relevance** | RÃ©pond Ã  la question posÃ©e | 1.0 |
| **Faithfulness** | Pas d'hallucination | 1.0 |

## ğŸ¤– Judge LLM

Le prompt du juge :
```
Evaluate the assistant answer.

Question: {question}
Expected: {expected}
Assistant: {answer}

Give scores between 0 and 1:
- groundedness
- relevance  
- faithfulness

Return JSON only.
```

## ğŸ“ˆ Sorties

- `eval_results.csv` : scores par question
- `dashboard.png` : moyennes (0â€“1) en graphique
- `history.csv` : historique des agrÃ©gats pour baseline

## âš™ï¸ Personnaliser

- Modifiez le dataset: `eval_dataset.json`
- Changez le Judge: Ã©ditez `judge_provider` / `judge_model` dans `run_evaluation()`
