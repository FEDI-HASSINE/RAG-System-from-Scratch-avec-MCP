# ğŸ” RAG System from Scratch avec MCP

Un systÃ¨me RAG (Retrieval-Augmented Generation) complet, construit de zÃ©ro avec architecture MCP (Model Context Protocol).

## ğŸ¯ Objectifs

- **Zero hallucination** : RÃ©ponses basÃ©es uniquement sur les documents
- **TraÃ§abilitÃ© complÃ¨te** : Chaque Ã©tape du pipeline est visible
- **Architecture modulaire** : Composants rÃ©utilisables via MCP
- **Multi-agent ready** : PrÃªt pour orchestration multi-agents

## ğŸ“¦ Phases du projet

| Phase | Dossier | Description |
|-------|---------|-------------|
| 1 | `data/` | Ingestion & chunking des documents |
| 2 | `embeddings/` + `vector_store/` | Vectorisation & indexation FAISS |
| 3-4 | `mcp_server/tools/` | Outils RAG (retrieve, rerank) |
| 5 | `mcp_server/` | API MCP unifiÃ©e |
| 6 | `agents/` | Agent RAG orchestrant le pipeline |
| 7 | `evaluation/` | LLM-as-a-Judge pour mÃ©triques |
| 8 | `demo/` | CLI Typer + UI Streamlit |

## ğŸš€ DÃ©marrage rapide

### 1. DÃ©marrer le serveur MCP
```bash
cd rag_system/mcp_server
uvicorn main:app --reload
```

### 2. Tester via CLI
```bash
cd rag_system/demo
python rag_cli.py ask "What is system architecture?"
```

### 3. Lancer l'interface web
```bash
cd rag_system/demo
streamlit run app.py
```

## ğŸ“ Structure

```
rag_system/
â”œâ”€â”€ data/              # Phase 1: Ingestion
â”‚   â”œâ”€â”€ raw_docs/      # Documents sources
â”‚   â”œâ”€â”€ loaders.py     # Chargement multi-format
â”‚   â”œâ”€â”€ chunker.py     # DÃ©coupage intelligent
â”‚   â””â”€â”€ chunks.json    # Sortie
â”œâ”€â”€ embeddings/        # Phase 2: Vectorisation
â”‚   â””â”€â”€ embedding_models.py
â”œâ”€â”€ vector_store/      # Phase 2: Stockage
â”‚   â”œâ”€â”€ faiss_store.py
â”‚   â””â”€â”€ index.faiss
â”œâ”€â”€ mcp_server/        # Phase 5: API MCP
â”‚   â”œâ”€â”€ main.py        # FastAPI
â”‚   â””â”€â”€ tools/         # Phases 3-4: Outils
â”œâ”€â”€ agents/            # Phase 6: Agent RAG
â”‚   â”œâ”€â”€ rag_agent.py
â”‚   â””â”€â”€ llm_service.py
â”œâ”€â”€ evaluation/        # Phase 7: MÃ©triques
â”‚   â””â”€â”€ eval_pipeline.py
â””â”€â”€ demo/              # Phase 8: Interfaces
    â”œâ”€â”€ rag_cli.py
    â””â”€â”€ app.py
```

## ğŸ”§ Technologies

| Composant | Technologie |
|-----------|-------------|
| Embeddings | SentenceTransformers (`all-MiniLM-L6-v2`) |
| Vector Store | FAISS |
| Reranking | Cross-Encoder (`ms-marco-MiniLM-L-6-v2`) |
| API | FastAPI + MCP |
| LLM | OpenAI / Ollama / Mock |
| CLI | Typer + Rich |
| UI | Streamlit |

## ğŸ“Š Pipeline RAG

```
Question â†’ Embed â†’ Retrieve â†’ Rerank â†’ Context â†’ LLM â†’ RÃ©ponse
              â”‚         â”‚          â”‚                    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      Tout exposÃ© via MCP
```

## ğŸ›¡ï¸ Zero Hallucination

Le systÃ¨me garantit des rÃ©ponses fiables :
- Prompts stricts imposant l'utilisation du contexte
- Citation explicite des sources
- Aveu d'ignorance si info absente

## ğŸ“– Documentation par dossier

Chaque dossier contient un `README.md` dÃ©taillant :
- ğŸ¯ Objectif
- ğŸ”§ ProblÃ¨mes rÃ©solus
- ğŸ“ Fichiers
- ğŸš€ Utilisation
